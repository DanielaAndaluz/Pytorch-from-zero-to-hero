{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469df154-137d-413f-a6fa-6521b2875ece",
   "metadata": {},
   "source": [
    "## üöÄ Level 8: Deployment\n",
    "\n",
    "### ‚úîÔ∏è Saving models\n",
    "- Save PyTorch models using `torch.save()` for state dict or entire model.\n",
    "- Example:\n",
    "  ```python\n",
    "  torch.save(model.state_dict(), \"model.pth\")  # save weights only\n",
    "  torch.save(model, \"model_full.pth\")           # save full model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475438f-98aa-4205-b386-f2c9b2f03771",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è Serving models with Flask / FastAPI\n",
    "- Use Flask or FastAPI to create REST APIs to serve model predictions.\n",
    "\n",
    "Example Flask snippet:\n",
    "  ```python\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = torch.load(\"model_full.pth\")\n",
    "model.eval()\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    inputs = torch.tensor(data[\"inputs\"])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    return jsonify(outputs.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d7a92-60a9-463b-842d-c9dd50809f4c",
   "metadata": {},
   "source": [
    "- FastAPI is similar but supports async and auto-generated docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbdaec2-d045-4869-8658-c124e736a193",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è ONNX & TorchScript for production\n",
    "- Export models for optimized production use.\n",
    "\n",
    "- TorchScript: Serialize models for use in C++ runtime or faster loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f09be5-6639-4a11-89e0-20536863ced0",
   "metadata": {},
   "source": [
    "  ```python\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"model_scripted.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d4578d-cbe0-4886-9c18-b9f8707bca83",
   "metadata": {},
   "source": [
    "ONNX: Export to ONNX format for interoperability with other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab823c-ebc1-4c3d-8b21-1d384b8ed926",
   "metadata": {},
   "source": [
    "```python\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e5978-cbbd-4479-8d55-97d3dbfefc31",
   "metadata": {},
   "source": [
    "| Concept                 | Description                                         | PyTorch Example / Tool                       |\n",
    "|-------------------------|-----------------------------------------------------|---------------------------------------------|\n",
    "| Saving Models           | Save model weights or full model for later use      | `torch.save(model.state_dict(), \"model.pth\")`<br>`torch.save(model, \"model_full.pth\")` |\n",
    "| Serving Models          | Create APIs to serve model predictions               | Flask, FastAPI                              |\n",
    "| ONNX & TorchScript      | Export models for optimized production deployment    | `torch.jit.script(model).save(\"model.pt\")`<br>`torch.onnx.export(model, dummy_input, \"model.onnx\")` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2beb49-8680-4628-9380-d0a58d0bd824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-macos)",
   "language": "python",
   "name": "tf-macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
