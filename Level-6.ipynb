{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c35b62-c477-48d1-8558-e8466e912760",
   "metadata": {},
   "source": [
    "# üõ† Level 6: Advanced Deep Learning\n",
    "\n",
    "In this level, you'll dive into powerful deep learning architectures used in **Computer Vision**, **Sequence Modeling**, and **Natural Language Processing (NLP)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úîÔ∏è Convolutional Neural Networks (CNNs)\n",
    "\n",
    "- CNNs are used primarily for **image data**.\n",
    "- They extract spatial features using **convolutional layers** and **pooling**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff80a9e-c23e-4489-8933-427d0dd3e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 14 * 14, 10)  # For 28x28 images like MNIST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd30cbc-d6bb-42d3-a73a-1a2b0942c2e4",
   "metadata": {},
   "source": [
    "‚úÖ CNNs are the backbone of tasks like image classification, object detection, segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a9a31f-1d24-43f7-adc6-8c37793aa5f7",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è RNNs & LSTMs (Recurrent Networks)\n",
    "- Used for sequential data: time series, text, speech.\n",
    "\n",
    "- LSTMs fix the vanishing gradient problem of vanilla RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3373e8f2-896c-426f-b988-92fc8a2a0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True)\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, batch_first=True)\n",
    "\n",
    "x = torch.randn(5, 3, 10)  # (batch, sequence, features)\n",
    "output, hn = lstm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcbce2f-4778-4607-b270-3cf7e58ef572",
   "metadata": {},
   "source": [
    "‚úÖ Used for text generation, language modeling, speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738576d8-b4a7-4f4a-a7a5-80599664a9e5",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è Attention & Transformers\n",
    "- Transformers use self-attention to model global relationships in sequences.\n",
    "\n",
    "- Replaced RNNs in NLP and now used in vision (ViTs).\n",
    "\n",
    "Minimal Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd3d682-1ef8-4d4f-9957-f4733e4fe110",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n",
    "query = key = value = torch.rand(10, 32, 512)  # (seq_len, batch, embed_dim)\n",
    "attn_output, attn_weights = attention(query, key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca2baa-688f-42fd-a47b-d00a674905c7",
   "metadata": {},
   "source": [
    "‚úÖ The Transformerarchitecture powers models like BERT, GPT, T5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a6edc-941b-43fd-804d-c2025a3758df",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Concept                     | PyTorch Example                                      | Use Case                                      |\n",
    "|-----------------------------|-----------------------------------------------------|-----------------------------------------------|\n",
    "| Convolutional Neural Networks | `nn.Conv2d`, `nn.MaxPool2d`, `nn.Linear`            | Image classification, detection              |\n",
    "| RNNs & LSTMs                 | `nn.RNN`, `nn.LSTM`                                 | Sequence modeling, time series, language      |\n",
    "| Attention & Transformers     | `nn.MultiheadAttention`, HuggingFace Transformers   | NLP, translation, summarization, Vision Transformers (ViTs) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bef765-c588-4e89-83f7-75b0b7493b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-macos)",
   "language": "python",
   "name": "tf-macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
